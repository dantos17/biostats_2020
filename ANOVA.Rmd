---
title: "ANOVA"
author: "Dani Antos"
date: "7/17/2020"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# One-Way ANOVA

While confusing, ANOVAs are really similar to linear models, basically a regression on indicator variables. The difference is the nature of the endpoint; ex. ANOVAs look at continuous variables as the response, logistic regressions use binary (0/1), Poisson uses counts (ex. does an animal respond to treatment, yes or no?), and proportional hazards regression look at survival endpoints. Before you run a test, consider your **output**. This doesn't really come into play for things like unsupervised clustering, just classical statistics. 

ANOVAs are like t-tests with **more than one class** and the *F-distribution* is a more generalized t-distribution. Instead of one null and one alternative hypothesis like in t-tests, there can be any number of hypotheses in ANOVAs. The **null hypothesis** is that none of the interventions (predictors) make any difference on the mean, while the **alternative hypothesis** states that at least one of the means with one of the predictors changes the mean (can be more than one). One of the outputs is the F statistic which is compared to the F distribution. The Classic ANOVA assumes that **the standard deviation in all of the class populations is the same**.

Where does the F statistic actually come from? 

* the **mean squared error** (MSE) is the averaged variance for all of the ANOVA classes (k, predictors); NOT the standard deviation, but this estimates the population standard deviation
* the **mean squares between effects** (MSB) takes the absolute population mean (grand mean) and calculates the averaged squared distance of each class mean from the population mean. If the indicators don't have an effect on the mean, the MSB will be very small; the larger the MSB is, the more the indicators actually make a difference in the endpoints. 
* the F statistic is MSB/MSE; if MSB is *large* (interventions are making a difference), the ratio will be high and the null hypothesis will be **rejected**
* the F distribution has two sets of degrees of freedom: numerator degrees of freedom *ndf* is calculated as k-1 and denominator degrees of freedom *ddf* is calculated as n-k

![](C:/Users/dania/Documents/Pitt/BioStats/Module 4/F-dist.jpg)

If F is above 4 (in the small tail), we will reject the null hypothesis. R and GraphPad will calculate the F statistic and give a p value so you don't actually need to know this specific distribution.

For our examples, we're using a dataset where not a ton is known but animals are either sick or healthy, and we're looking at MFI of macrophages, pDCs, and mDCs with CD40 or CD80 as costimulatory molecules.
```{r reading data in}
setwd("C:/Users/dania/Documents/Pitt/BioStats/Module 4")
z <- read.table("fig4b.txt",sep="\t",header=T, stringsAsFactors = T)
head(z)
plot(z$cell.type,z$mfi)
library(ggplot2)
plot.z <-ggplot(z, aes(x=cell.type, y=mfi)) + 
  geom_dotplot(binaxis='y', stackdir='center')
plot.z #not what daniel used but he had a user function
#from the data we can see that pDCs have a way lower MFI than the other two cell types
```

In the output we are looking for:

* MSB = Mean sq(group)
* MSE = Residual standard error, sigma^2
* F = MSB/MSE
* ndf = Df(group)
* ddf = Df(Residuals)
* p value = Pf(>F), 1-pf(F value, Df(group), Df(Residuals))

```{r}
lm.1 <- lm(mfi~cell.type,data=z) #can also use aov() function to get ANOVA table automatically
summary(lm.1) #macrophages are taken as the reference aka base level, and then pDCs and mDCs are the indicator variables -- you can change this to make the control group the reference group
anova(lm.1)

qqnorm(lm.1$residuals,col="navy")
qqline(lm.1$residuals)

```

In the summary of lm.1, the estimate refers to the MFI mean for the designated population. Because the reference level is the macrophages, the mean MFI for that population is given and then the mean MFI for the other two populations is given in relation to the macrophages. The p value for macrophages is just a test to make sure the estimate is different from 0. the mDC p value is not significant because the means are close, and the pDC p value is significant because the mean of macs and pDCs is not close at all. The summary also gives a p value for the entire model next to the F statistic. You have to be careful in using these p values though, because each p value takes the other indicator variables into account when being calculated, for example: the **pDC p value** is conditional on having the other two cell types in the model. 

The *ANOVA* table solves this problem by only including a p value for cell type as a whole. The p value is for the null hypothesis that ALL the cell types are the same, which they clearly aren't. The one-way ANOVA is a generalization of the t-test because it doesn't say which cell type is different or driving the difference. 

Now we can check the qqplot for normality of the data. There are outliers here, which can mean one of three things:

* there are interactions in the dataset that aren't being considered
* the relationships aren't linear, need a more complicated model
* there are variables in the dataset that make a difference but aren't being considered in the current model
* it could also be a situation of a variable not being considered AT ALL; for example, if sex is important to how a drug works but the males and females are lumped together and not noted anywhere in the dataset

The conclusion either way given the residuals plot tells us that the one-way ANOVA is too simple to provide a good fit given the model and the dataset.

## Welch's Variant

As with t tests, there is a **Welch's variant** for the one way ANOVA that will adjust the degrees of freedom in case the standard deviation changes among groups. For this example, it looks like the pDC SD is much smaller than the other two, so Welch's variant would be a better option. It is often found that the mean is *proportional to the standard deviation*- if you have a higher mean, you get a larger spread. Welch's variant, the Kruskal-Wallis test, or a log transformation can fix this.

```{r Welch variant}
oneway.test(mfi~cell.type,data=z)
```

## Kruskal-Wallis Test

If the distribution within the classes is non-Gaussian, especially when sample size is small, the non-parametric **Kruskal-Wallis** test is a good alternative. It's basically like using the Wilcoxon test as a t test alternative. If the data *is* Gaussian, using this test will result in a loss of some power.

```{r Kruskal-Wallis}
kruskal.test(mfi~cell.type,data=z) #rank based test
```

**General workflow for running an ANOVA**:

1. Run *anova()* and look at the summary and qqplot for normality
2. If qqplot has outliers, run the one-way ANOVA with Welch's variant, *oneway.test()*, which does not assume homoscedasticity (same SD for all classes)
3. If non-Gaussian, use Kruskal-Wallis test, *kruskal.test()*

## Post-Hoc Comparisons

If we reject the null hypothesis that ALL of the means are the same, then we are left to figure out which means are different, which can mean lots of pairwise comparisons depending on the number of classes (k). **Multiple comparison adjustments** can be made to lower the possibility of rejecting the null hypothesis randomly just because there are too many pairwise tests. This is not necessary if the F statistic does not correspond with rejecting the null hypothesis. 

### Bonferroni Adjustment

If you have "*m*" pairwise null hypotheses and want to test them at significance level *a* (alpha), multiply each p value by m. If the new p value is less than alpha, then it is significant.

This adjustment is easy to perform but it's incredibly conservative, meaning that some pairwise tests could actually be significant in real time but cut out by the Bonferroni adjustment.

### Tukey's Adjustment

This is more widely accepted, requires the {multcomp} package under the glht() function. GLHT stands for general linear hypothesis testing, and the multcomp package has a lot of good functions for this general statistical idea. These will be more protected than doing 3 separate t tests.

```{r Tukey}
require(multcomp)
lm.1.tukey <- glht(lm.1,linfct = mcp(cell.type="Tukey")) #designate which group needs the adjustment
summary(lm.1.tukey)
```

### Dunnett's Adjustment

If you have a single control that's clearly the control that you want the other classes to be compared to, use the Dunnett's adjustment. It's less conservative than Tukey's adjustment, but the control group has to be identified with the *relevel()* function. 

```{r Dunnett}
z$cell.type <- relevel(z$cell.type,"pDC")

lm.1 <- lm(mfi~cell.type-1,data=z)
lm.1.dunnett <- glht(lm.1,linfct = mcp(cell.type="Dunnett")) #still have to say which group needs to be adjusted
summary(lm.1.dunnett) #in the output, the control group is always the second term in the subtraction
```

# Two-Way ANOVA

Again, the endpoint or response variable is continuous and the predictors can be categorical (ex. Drug 1 and Drug 2). For the t-test and the one-way ANOVA, there was one categorical variable with 2 (t-test) or 3 (ANOVA) possible values, and for the two-way ANOVA, there are **two** categorical variables. Using Drug 1 and Drug 2 as an example, there are 4 different combinations of treatment that can each have a different outcome. 

The *null hypothesis* is always the same: are the *means* of each treatment group the **same**? Aka, neither drug will make a difference. The other questions we can ask are whether Drug 1 or Drug 2 make a difference in the model or if the two drugs work in synergy.

## Additive Model

The **additive model** is one explanation of the parameters on the model. In this drug example, there are four relevant equations that correspond to each treatment group:

* mean(control) = B0
* mean(Drug 1) = B0 + B1; B1 is the effect of Drug 1 on Y
* mean(Drug 2) = B0 + B2; B2 is the effect of Drug 2 on Y
* mean(combo) = B0 + B1 + B2; assuming that the effect of both drugs is the same as adding the effect of each drug alone, which may NOT always be true

```{r example}
set.seed(2345)
#creating the effects of the drugs using vectors
d1 <- c(rep(0,20),rep(1,20))
d2 <- c(rep(0,10),rep(1,10),rep(0,10),rep(1,10))
y <- 100+10*d1+15*d2+7*d1*d2+rnorm(length(d1),0,20) #simulation model with 100 animals mean; the numbers are the B values and rnorm is the noise component
d1 <- as.factor(d1)
d2 <- as.factor(d2)
table(d1,d2)

lm.5 <- lm(y~d1*d2) #asterisk means that we want each effect separately PLUS d1*d2; we don't actually care about the coefficients
anova(lm.5)
summary(lm.5)
```
From the ANOVA table, we can see that the effect of each d1 and d2 are statistically significant, but the interaction of the two is not. In this case, we do know that the interaction actually is significant, but the stochastic component that we included has a lot of noise, which is what actually takes significance away from the interaction. If looking for synergistic effects is the point of the experiment, looking at the noise component is SUPER important. 
```{r switching predictor input}
library(car)
Anova(lm.5,type=2)
#Daniel got very confused here, but Anova gives conditional p values not sequential

lm.6 <- lm(y~d2*d1)
anova(lm.6) #switching d1 and d2 and running a sequential anova may give different significance values, but it doesn't here since the experiment is balanced
```

Using the data from the one way ANOVA, we can run the same type of tests with a more complicated dataset. Here, there are 4 types of co-stimulatory markers (CD40, CD80, CD86, CD274) and 3 types of cells (macrophages, pDCs, mDCs). 

```{r}
lm.2 <- lm(mfi~cell.type+co.stim.mol,data=z) #same as one-way with the addition of costimulatory markers
summary(lm.2)
anova(lm.2)
```

Daniel's code shows macrophages as being the reference, but here, the pDCs are the reference level. Again, the coefficients are all in relation to the reference. In the summary, the meaningful data are the R-squared values and the p-value for the whole model. The adjusted R-squared tells us that 65% of the variation is explained by the model, a significant percentage, and the whole model is significant. The individual numbers for each group are not necessarily important. 

The ANOVA gives more meaningful information. This goes off of the idea that the co-stimulatory molecule and cell type are completely independent. For example, the co-stimulatory molecule present will not impact the cell type. The ANOVA doesn't tell you anything about the effect on cell type on MFI if you *know* the co-stimulatory molecule. It also doesn't tell us in which direction the effect is (positive or negative).
```{r}
qqnorm(lm.2$residuals,col="navy")
qqline(lm.2$residuals)
```

The qqplot is pretty good, but there are a few outliers as we can see from the graph. This could again mean that there are additional variables that we are not accounting for in our model, but you can't tell that from just the graph. From the graph, there are some observations that are *larger* than we would expect. 

## Model with Interactions

Another model to use instead of the additive model is the model with *interactions*. Using the simple drug example, here are the equations needed to explain the parameters in the model:

* mean(control) = B0
* mean(Drug 1) = B0 + B1; B1 is the effect of Drug 1 on Y
* mean(Drug 2) = B0 + B2; B2 is the effect of Drug 2 on Y
* mean(combo) = B0 + B1 + B2 + B3; each of the drugs has their own effect on Y and B3 is the effect of the interaction of both drugs on Y

Given these set of equations, if **B3=0**, there is additivity, meaning that putting the two drugs together is the same as just adding their individual effects together. If **B3>0**, there is synergism, meaning that putting the drugs together has a greater effect than just the individual effects together. If **B3<0**, there is antagonism, meaning that the combination of drugs decreases the effect that you would expect by adding the individual effects together. In order to claim synergy, *B3>0 and statistically significant*. 

**Interaction plots** can be visual aids in determining interactions between the predictors. 

![](C:/Users/dania/Documents/Pitt/BioStats/Module 4/interaction.jpg)

On the left, there is no interaction between Drug 1 and Drug 2 as shown by the parallel lines. Giving Drug 2 always increases the mean regardless of whether or not Drug 1 is present. On the right, Drug 3 has a suppressive effect on Drug 4. Drug 3 without Drug 4 does nothing, but the impact of Drug 4 in the presence of Drug 3 is greatly diminished. 

Taking this info, we can apply it to our real example with cell types, co-stimulatory molecules, and MFI.

```{r}
lm.3 <- lm(mfi~cell.type*co.stim.mol,data=z)
summary(lm.3)
anova(lm.3)

qqnorm(lm.3$residuals,col="navy")
qqline(lm.3$residuals)
```

The output here is comparing all other conditions to the reference, in this case being pDCs and CD274. The reference can be thought of as the "baseline," and the others are in relation to that; cell type and the co-stimulatory molecules are the main effects, and the others are the interactions. The R-squared tells us that 73% of the variation in MFI is due to the cell type, co-stimulatory molecules, and the interactions between the two. The significance of the effect is also highly highly significant, indicating that we believe that these effects are not due to random variation. The ANOVA table tells us that both cell type and co-stimulatory molecules are significant predictors, but also that the interaction is significant. This means that knowing something about the cell type will tell you something about how the co-stimulatory molecule will relate to the MFI and vice versa. From this data, you CAN'T imply causation. 

So what's the difference between a linear model and an ANOVA? *lm()* will describe the parameters and their coefficients while *anova()* describes the relationship between the variables. The difference becomes more important when the predictor has more than two categories (ex. more than yes/no). In ANOVAs, it is super important to be conscious of whether the p-values are conditional or sequential (Anova() vs anova()). 

There can also be adjustments made for a predictor, which is essentially just subtracting out a mean. This is common in stats but you have to be careful.

**Balanced vs Unbalanced experiments**: In our examples we've talked about having two drugs. A balanced experiment has the same number of observations in every experimental group and will give the most statistical power. This is pretty intuitive, but sometimes this can go wrong if animals die, etc. In an unbalanced experiment, the sequence of effects in the model can influence the results. If the experiment is unbalanced, it's not the end of the world, but you **have** to know if the p values are conditional or sequential. 